{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A short & practical introduction to Tensor Flow!\n",
    "\n",
    "Part 6\n",
    "\n",
    "Example of a single-layer one-directional long short-term memory network (LSTM) trained with\n",
    "[connectionist temporal classification](http://www.cs.toronto.edu/~graves/icml_2006.pdf) to predict character sequences from nFeatures x nFrames\n",
    "arrays of Mel-Frequency Cepstral Coefficients.  This is test code to run on the\n",
    "8-item data set in the \"sample_data\" directory, for those without access to TIMIT.\n",
    "\n",
    "Author: [Jon Rein](https://github.com/jonrein/tensorflow_CTC_example) \n",
    "\n",
    "Adapted by: Pablo M. Olmos (olmos@tsc.uc3m.es)\n",
    "\n",
    "Date: March 2017\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from tensorflow.python.ops import ctc_ops as ctc\n",
    "import numpy as np\n",
    "from utils import load_batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets check what version of tensorflow we have installed. The provided scripts should run with tf 1.0 and above\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchSize = 4 # Batch of sequences\n",
    "\n",
    "## Change according to the folder where you saved the dataset provided\n",
    "INPUT_PATH = '../../DataSets/MCC_sample_data_phoneme_recog/mfcc' #directory of MFCC nFeatures x nFrames 2-D array .npy files\n",
    "TARGET_PATH = '../../DataSets/MCC_sample_data_phoneme_recog/char_y/' #directory of nCharacters 1-D array .npy files\n",
    "\n",
    "\n",
    "####Load data\n",
    "print('Loading data')\n",
    "batchedData, maxTimeSteps, totalN = load_batched_data(INPUT_PATH, TARGET_PATH, batchSize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Learning Parameters\n",
    "learningRate = 0.001\n",
    "momentum = 0.9\n",
    "nEpochs = 200\n",
    "\n",
    "####Network Parameters\n",
    "nFeatures = 26 #12 MFCC coefficients + energy, and derivatives\n",
    "nHidden = 128\n",
    "nClasses = 28#27 characters, plus the \"blank\" for CTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the computation graph\n",
    "\n",
    "We will create a LSTM layer with 128 memory cells. On top of this, we use a fully connected soft-max layer.\n",
    "\n",
    "We use [CTC classification](http://www.cs.toronto.edu/~graves/icml_2006.pdf) to compute the loss function that we can optimize by gradient descend. This function is already provided in the TF [contributions library](        https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/conectionist_temporal_classification__ctc_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs \n",
    "\n",
    "Recall the fundamental model\n",
    "\n",
    "\n",
    "<img src=\"files/figLSTM.png\">\n",
    "\n",
    "Also, the un-regularized cost function is\n",
    "\n",
    "\\begin{align}\n",
    "J(\\boldsymbol{\\theta})=\\frac{1}{N}\\sum_{n=1}^N\\sum_{t=1}^{T_n}d(\\boldsymbol{y}_t^{(n)},\\text{softmax}(\\boldsymbol{W}_h \\boldsymbol{h}_t^{(n)}+\\mathbf{b}))\n",
    "\\end{align}\n",
    "where $d(\\cdot,\\cdot)$ is the cross-entropy loss function.\n",
    "\n",
    "### Bi-directional LSTMs \n",
    "\n",
    "\\begin{align}\n",
    "J(\\boldsymbol{\\theta})=\\frac{1}{N}\\sum_{n=1}^N\\sum_{t=1}^{T_n}d(\\boldsymbol{y}_t^{(n)},\\text{softmax}(\\boldsymbol{W}_h \\boldsymbol{h}_t^{(n)}+\\boldsymbol{W}_h \\boldsymbol{z}_t^{(n)}+\\mathbf{b})),\n",
    "\\end{align}\n",
    "where $\\boldsymbol{z}_t^{(n)}$ is the output at time t of the backward LSTM NN. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####Define graph\n",
    "print('Defining graph')\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    \n",
    "\n",
    "    #### We start by encoding the forward LSTM with nHidden cells\n",
    "    \n",
    "    \n",
    "    #i(t) parameters\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([nFeatures, nHidden], -0.05, 0.05))   ##W^ix\n",
    "    im = tf.Variable(tf.truncated_normal([nHidden, nHidden], -0.05, 0.05)) ## W^ih\n",
    "    ib = tf.Variable(tf.zeros([1, nHidden])) ##b_i\n",
    "    \n",
    "    #f(t) parameters\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([nFeatures, nHidden], -0.05, 0.05)) ##W^fx\n",
    "    fm = tf.Variable(tf.truncated_normal([nHidden, nHidden], -0.05, 0.05)) ##W^fh\n",
    "    fb = tf.Variable(tf.zeros([1, nHidden])) ##b_f\n",
    "    \n",
    "    #g(t) parameters\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([nFeatures, nHidden], -0.05, 0.05)) ##W^gx\n",
    "    cm = tf.Variable(tf.truncated_normal([nHidden, nHidden], -0.05, 0.05)) ##W^gh\n",
    "    cb = tf.Variable(tf.zeros([1, nHidden]))  ##b_g\n",
    "    \n",
    "    #o(t) parameters\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([nFeatures, nHidden], -0.05, 0.05))  ##W^ox\n",
    "    om = tf.Variable(tf.truncated_normal([nHidden, nHidden], -0.05, 0.05))  ##W^oh\n",
    "    ob = tf.Variable(tf.zeros([1, nHidden])) ##b_o\n",
    "    \n",
    "    # Variable saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batchSize, nHidden]), trainable=False) #h(t)\n",
    "    saved_state = tf.Variable(tf.zeros([batchSize, nHidden]), trainable=False) #s(t)\n",
    "    \n",
    "    ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  #### \n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb       \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)    #tf.tanh(update) is g(t)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state      #h(t) is output_gate * tf.tanh(state) \n",
    "    \n",
    "        \n",
    "    #### Now the backward LSTM with nHidden cells\n",
    "    \n",
    "    #i(t) parameters\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    b_ix = tf.Variable(tf.truncated_normal([nFeatures, nHidden], -0.05, 0.05))   ##W^ix\n",
    "    b_im = tf.Variable(tf.truncated_normal([nHidden, nHidden], -0.05, 0.05)) ## W^ih\n",
    "    b_ib = tf.Variable(tf.zeros([1, nHidden])) ##b_i\n",
    "    \n",
    "    #f(t) parameters\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    b_fx = tf.Variable(tf.truncated_normal([nFeatures, nHidden], -0.05, 0.05)) ##W^fx\n",
    "    b_fm = tf.Variable(tf.truncated_normal([nHidden, nHidden], -0.05, 0.05)) ##W^fh\n",
    "    b_fb = tf.Variable(tf.zeros([1, nHidden])) ##b_f\n",
    "    \n",
    "    #g(t) parameters\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    b_cx = tf.Variable(tf.truncated_normal([nFeatures, nHidden], -0.05, 0.05)) ##W^gx\n",
    "    b_cm = tf.Variable(tf.truncated_normal([nHidden, nHidden], -0.05, 0.05)) ##W^gh\n",
    "    b_cb = tf.Variable(tf.zeros([1, nHidden]))  ##b_g\n",
    "    \n",
    "    #o(t) parameters\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    b_ox = tf.Variable(tf.truncated_normal([nFeatures, nHidden], -0.05, 0.05))  ##W^ox\n",
    "    b_om = tf.Variable(tf.truncated_normal([nHidden, nHidden], -0.05, 0.05))  ##W^oh\n",
    "    b_ob = tf.Variable(tf.zeros([1, nHidden])) ##b_o\n",
    "    \n",
    "    # Variable saving state across unrollings.\n",
    "    b_saved_output = tf.Variable(tf.zeros([batchSize, nHidden]), trainable=False) #h(t)\n",
    "    b_saved_state = tf.Variable(tf.zeros([batchSize, nHidden]), trainable=False) #s(t)\n",
    "    \n",
    "    \n",
    "    # Definition of the backward_cell computation.\n",
    "    def lstm_cell_back(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, b_ix) + tf.matmul(o, b_im) + b_ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, b_fx) + tf.matmul(o, b_fm) + b_fb)\n",
    "        update = tf.matmul(i, b_cx) + tf.matmul(o, b_cm) + b_cb       \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)    #tf.tanh(update) is g(t)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, b_ox) + tf.matmul(o, b_om) + b_ob)\n",
    "        return output_gate * tf.tanh(state), state      #h(t) is output_gate * tf.tanh(state) \n",
    "    \n",
    "    \n",
    "    ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  #### \n",
    "    \n",
    "    # Classifier weights and biases (over h(t) and b_h(t) to labels)\n",
    "    w = tf.Variable(tf.truncated_normal([nHidden, nClasses], -0.05, 0.05))\n",
    "    b = tf.Variable(tf.zeros([nClasses]))    \n",
    "    b_w = tf.Variable(tf.truncated_normal([nHidden, nClasses], -0.05, 0.05))\n",
    "\n",
    "    ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  #### \n",
    "    \n",
    "    # Now we define the placeholders for the input data\n",
    "        \n",
    "    train_data = list()\n",
    "    for _ in range(maxTimeSteps):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batchSize,nFeatures]))\n",
    "\n",
    "    targetIxs = tf.placeholder(tf.int64)\n",
    "    targetVals = tf.placeholder(tf.int32)\n",
    "    targetShape = tf.placeholder(tf.int64)\n",
    "    targetY = tf.SparseTensor(targetIxs, targetVals, targetShape)\n",
    "    seqLengths = tf.placeholder(tf.int32, shape=(batchSize))    \n",
    "        \n",
    "    # Given the input, we indicate how to compute the hidden states  \n",
    "        \n",
    "    # Unrolled forward LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_data:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "        \n",
    "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "\n",
    "        # Unrolled backward LSTM loop. Initilialized with the last state of the forward loop!!\n",
    "        # With the control dependencies command, we make sure we do not run the backward unrolling\n",
    "        # until the forward one is finished.\n",
    "        \n",
    "        b_outputs = list()\n",
    "        b_output = output\n",
    "        b_state = state\n",
    "        for i in reversed(train_data):\n",
    "            b_output, b_state = lstm_cell_back(i, b_output, b_state)\n",
    "            b_outputs.append(b_output)        \n",
    "        \n",
    "        b_outputs=b_outputs[::-1]\n",
    "\n",
    "        with tf.control_dependencies([b_saved_output.assign(b_output),b_saved_state.assign(b_state)]):        \n",
    "\n",
    "            logits = tf.reshape(tf.matmul(tf.concat(axis=0,values=outputs),w)+\n",
    "                                tf.matmul(tf.concat(axis=0,values=b_outputs),b_w)+b,[-1,batchSize,nClasses]) \n",
    "\n",
    "            #https://www.tensorflow.org/versions/r0.050/api_docs/python/nn/conectionist_temporal_classification__ctc_\n",
    "            loss = tf.reduce_mean(ctc.ctc_loss(inputs=logits, labels=targetY,  sequence_length=seqLengths))\n",
    "\n",
    "                ####Optimizing\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate, momentum).minimize(loss)\n",
    "\n",
    "\n",
    "    ####Evaluating\n",
    "    logitsMaxTest = tf.slice(tf.argmax(logits, 2), [0, 0], [seqLengths[0], 1])\n",
    "    predictions = tf.to_int32(ctc.ctc_beam_search_decoder(logits, seqLengths)[0][0])\n",
    "    errorRate = tf.reduce_sum(tf.edit_distance(predictions, targetY, normalize=False)) / tf.to_float(tf.size(targetY.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####Run session\n",
    "with tf.Session(graph=graph) as session:\n",
    "    print('Initializing')\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(nEpochs):\n",
    "        print('Epoch', epoch+1, '...')\n",
    "        batchErrors = np.zeros(len(batchedData))\n",
    "        batchRandIxs = np.random.permutation(len(batchedData)) #randomize batch order\n",
    "        for batch, batchOrigI in enumerate(batchRandIxs):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = batchedData[batchOrigI]\n",
    "            batchTargetIxs, batchTargetVals, batchTargetShape = batchTargetSparse\n",
    "            feedDict = {targetIxs: batchTargetIxs, targetVals: batchTargetVals,\n",
    "                        targetShape: batchTargetShape, seqLengths: batchSeqLengths}\n",
    "            for i in range(maxTimeSteps):\n",
    "                feedDict[train_data[i]] = batchInputs[i,:,:]\n",
    "                \n",
    "            _, l, er, lmt,logits_out = session.run([optimizer, loss, errorRate, logitsMaxTest,logits], feed_dict=feedDict)\n",
    "            print(np.unique(lmt)) #print unique argmax values of first sample in batch; should be blank for a while, then spit out target values\n",
    "            if (batch % 1) == 0:\n",
    "                print('Minibatch', batch, '/', batchOrigI, 'loss:', l)\n",
    "                print('Minibatch', batch, '/', batchOrigI, 'error rate:', er)\n",
    "            batchErrors[batch] = er*len(batchSeqLengths)\n",
    "        epochErrorRate = batchErrors.sum() / totalN\n",
    "        print('Epoch', epoch+1, 'error rate:', epochErrorRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets visualize the prediction for the first training sequence\n",
    "np.argmax(logits_out[:,0,:],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(logits_out[:,0,27],'k-',label='blank')\n",
    "plt.plot(logits_out[:,0,10],'b-',label='Index 10')\n",
    "plt.plot(logits_out[:,0,9],'r-',label='Index 9')\n",
    "plt.plot(logits_out[:,0,26],'g-',label='Index 12')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Output Logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about the spykes that tend to apear at the RNN output when using CTC \n",
    "\n",
    "[Supervised Sequence Labelling](https://www.cs.toronto.edu/~graves/preprint.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
