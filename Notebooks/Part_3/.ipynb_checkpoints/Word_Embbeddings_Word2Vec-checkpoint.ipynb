{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A short & practical introduction to Tensor Flow!\n",
    "\n",
    "Part 3\n",
    "\n",
    "The goal of this assignment is to train a Word2Vec skip-gram model over [Text8](http://mattmahoney.net/dc/textdata) data.\n",
    "\n",
    "This is a personal wrap-up of all the material provided by [Google's Deep Learning course on Udacity](https://www.udacity.com/course/deep-learning--ud730), so all credit goes to them. \n",
    "\n",
    "Author: Pablo M. Olmos (olmos@tsc.uc3m.es)\n",
    "\n",
    "Date: March 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings using the Word2Vec skp-gram model\n",
    "\n",
    "The following [link](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) gives a very simple explanation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Lets check what version of tensorflow we have installed. The provided scripts should run with tf 1.0 and above\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from the source website if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ../../DataSets/textWordEmbeddings/text8.zip\n"
     ]
    }
   ],
   "source": [
    "filename = preprocessing.maybe_download('../../DataSets/textWordEmbeddings/text8.zip', 31344016) ## Change according to the folder where you saved the dataset provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Read the data into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "words = preprocessing.read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n"
     ]
    }
   ],
   "source": [
    "print(words[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dictionary and replace rare words with UNK token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = preprocessing.build_dataset(vocabulary_size,words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the internal variables to better understand their structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n",
      "[['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])\n",
    "print(count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('UNK', 0), ('the', 1), ('of', 2), ('and', 3), ('one', 4), ('in', 5), ('a', 6), ('to', 7), ('zero', 8), ('nine', 9)]\n",
      "[(0, 'UNK'), (1, 'the'), (2, 'of'), (3, 'and'), (4, 'one'), (5, 'in'), (6, 'a'), (7, 'to'), (8, 'zero'), (9, 'nine')]\n"
     ]
    }
   ],
   "source": [
    "print(list(dictionary.items())[:10])\n",
    "print(list(reverse_dictionary.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the word dictionary is 39018\n",
      "\n",
      "The word corresponding to the index 875 is edition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The index of the word dictionary is %d\\n' %(dictionary['crafty']))\n",
    "print('The word corresponding to the index 875 is %s\\n' %(reverse_dictionary[875]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'UNK', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term']\n",
      "\n",
      "with num_skips = 2 and skip_window = 4:\n",
      "    batch: ['term', 'term', 'of', 'of', 'abuse', 'abuse', 'first', 'first', 'used', 'used', 'against', 'against', 'early', 'early', 'working', 'working']\n",
      "    labels: ['as', 'of', 'a', 'as', 'against', 'term', 'early', 'working', 'first', 'early', 'first', 'radicals', 'abuse', 'used', 'first', 'against']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "\"\"\"Generate a batch of data for training.\n",
    "    Args:\n",
    "        batch_size: Number of samples to generate in the batch.\n",
    "        \n",
    "        skip_window:# How many words to consider left and right.\n",
    "        \n",
    "            How many words to consider around the target word, left and right.\n",
    "            With skip_window=2, in the sentence above for \"consider\" we'll\n",
    "            build the window [words, to, consider, around, the].\n",
    "            \n",
    "        num_skips: How many times to reuse an input to generate a label.\n",
    "        \n",
    "            For skip-gram, we map target word to adjacent words in the window\n",
    "            around it. This parameter says how many adjacent word mappings to\n",
    "            add to the batch for each target word. Naturally it can't be more\n",
    "            than skip_window * 2.\n",
    "            \n",
    "    Returns:\n",
    "        batch, labels - ndarrays with IDs.\n",
    "        batch: Row vector of size batch_size containing target words.\n",
    "        labels:\n",
    "            Column vector of size batch_size containing a randomly selected\n",
    "            adjacent word for every target word in 'batch'.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:32]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 4)]:\n",
    "    data_index = 0\n",
    "    batch, labels = preprocessing.generate_batch(data, data_index, batch_size=16, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(16)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the above data set, now we train a skip-gram model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following [link](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) gives a very simple explanation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 32 # Random set of words to evaluate similarity on.\n",
    "valid_window = 200 # Only pick samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs. YOU DON'T NEED THE ONE HOT ENCODING FOR THE INPUT!!!! :)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, train_labels, \n",
    "                                                     embed, num_sampled, vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 7.633489\n",
      "Nearest to while: rate, rummel, rehearsed, rsa, submarine, decisive, ezekiel, constantius,\n",
      "Nearest to high: biogeography, profoundly, seem, rp, splines, cryptographers, marcian, visceral,\n",
      "Nearest to another: inquisitors, accusing, manny, otherwise, fiscal, strategic, subcontinent, mysteries,\n",
      "Nearest to him: keratin, genital, certainty, batmobile, bringer, magically, semifinals, harboring,\n",
      "Nearest to people: minya, denies, disco, spherically, anecdotes, tortuous, restaurants, volk,\n",
      "Nearest to than: abrahams, mir, eastman, dtds, mamadou, cuyahoga, respectively, paintings,\n",
      "Nearest to at: stanbul, handicapped, advising, gruyter, putatively, promulgated, hampshire, episcopacy,\n",
      "Nearest to states: affirmation, electrode, noise, tubules, montmartre, alnus, proletarian, compendium,\n",
      "Nearest to being: jackal, directories, neighbor, xvii, alkmaar, jessica, excluded, savoy,\n",
      "Nearest to world: iq, cmyk, fucking, palm, hamill, decompression, schala, bonn,\n",
      "Nearest to and: andree, caribs, cor, libby, winding, warehouse, weep, cto,\n",
      "Nearest to music: patient, sprung, universitet, hesse, agency, kilobytes, baguirmi, transducers,\n",
      "Nearest to work: maneuverable, iole, bukhara, rohe, mathworld, gardeners, infield, seconds,\n",
      "Nearest to will: minority, christo, orchestration, polos, stallone, moltke, passy, vineyards,\n",
      "Nearest to n: lentils, monogamous, matching, pharisees, wikibooks, erotica, gained, willamette,\n",
      "Nearest to that: orthogonality, insolvency, sectional, amend, tavistock, orca, gli, polyatomic,\n",
      "Nearest to from: bob, biola, ann, blish, philosophie, appletalk, fading, renowned,\n",
      "Nearest to term: campos, designs, structuring, hennepin, nightmare, durations, gradually, gpp,\n",
      "Nearest to the: assesses, embody, unqualified, headstone, austrofascism, proper, persistent, spared,\n",
      "Nearest to about: courtiers, zealot, eerie, teams, australasian, bubbles, woefully, overthrow,\n",
      "Nearest to common: alaric, cask, thoroughbreds, versace, svante, portuguese, discipline, marching,\n",
      "Nearest to e: honeydew, epa, heroine, researching, parnell, gaku, torturing, hummers,\n",
      "Nearest to state: milled, shoes, whammy, deseret, commissioner, derivative, horton, korean,\n",
      "Nearest to day: waxes, fta, pass, decidedly, stanislaw, welterweight, tuning, taverns,\n",
      "Nearest to what: veterans, gohan, looters, postalveolar, awaits, plausibly, romney, panza,\n",
      "Nearest to into: credibility, putnam, barrett, despise, feud, breakdancing, walkers, notified,\n",
      "Nearest to list: overlooking, truro, electrolyte, bawdy, jass, colder, boyz, sabina,\n",
      "Nearest to still: antiderivatives, computational, unrequited, marginal, suspecting, sunshine, titus, bruin,\n",
      "Nearest to power: lilongwe, pyrrhus, silurian, bindings, heidelberg, chants, vishnu, catamarans,\n",
      "Nearest to became: cgpm, amour, shakespearean, allophonic, troodos, morphological, moldavian, sprite,\n",
      "Nearest to general: doorway, chime, mckeon, havenco, podium, micronesian, crops, melon,\n",
      "Nearest to century: subversion, crustacean, bookstores, kinsman, granada, trainer, smoothness, eller,\n",
      "Average loss at step 2000: 4.641339\n",
      "Average loss at step 4000: 4.146368\n",
      "Average loss at step 6000: 4.011093\n",
      "Average loss at step 8000: 3.796085\n",
      "Average loss at step 10000: 3.807811\n",
      "Nearest to while: international, persona, hanseatic, aligned, between, mandy, archtop, after,\n",
      "Nearest to high: disorders, tyr, himachal, estonian, sto, splines, repressive, ctv,\n",
      "Nearest to another: weapons, cheng, ambrosius, helical, chennai, halfbakery, kurz, sap,\n",
      "Nearest to him: howlin, genital, pittsburgh, nernst, keratin, toned, redshift, soter,\n",
      "Nearest to people: men, alonzo, neonatal, gabriel, amiga, killed, cricketers, hinduism,\n",
      "Nearest to than: impress, sketched, compatibles, considerably, orphaned, liquefaction, heed, depositing,\n",
      "Nearest to at: from, moat, in, of, on, hittite, legionaries, triglycerides,\n",
      "Nearest to states: vci, lc, electrode, nuance, operator, belo, noise, satisfying,\n",
      "Nearest to being: jackal, signature, roman, neighbor, priority, administering, subsidized, thessalonians,\n",
      "Nearest to world: hs, mann, fucking, connects, mercosur, vehicles, intervocalic, tatars,\n",
      "Nearest to and: he, was, mazes, symbolised, s, for, chen, newcomers,\n",
      "Nearest to music: patient, fachhochschule, kilobytes, complement, hesse, universitet, museo, stalls,\n",
      "Nearest to work: accountant, synonymously, surface, consulate, infield, disciplines, bleed, maneuverable,\n",
      "Nearest to will: can, would, that, ballot, thrice, generally, costing, polos,\n",
      "Nearest to n: c, electors, sharper, necromancer, authoritarian, wikibooks, catnip, terrified,\n",
      "Nearest to that: which, also, he, will, adaptable, antoni, colons, pretty,\n",
      "Nearest to from: at, monoamine, into, with, in, xaver, terri, mohamed,\n",
      "Nearest to term: probabilities, taunting, clitoridectomy, nonpolar, ambiguous, oceans, resolve, aloysius,\n",
      "Nearest to the: choral, their, his, an, its, imperceptible, a, escalate,\n",
      "Nearest to about: resupply, pilgrim, gown, chimes, michigan, ins, burghs, tribunals,\n",
      "Nearest to common: thoroughbreds, agnostic, simone, subtype, certainty, fatwa, kitchener, generalized,\n",
      "Nearest to e: epa, yousef, seminaries, systematic, honeydew, apatosaurus, heroine, succeeded,\n",
      "Nearest to state: tests, busch, sushi, possesses, shank, announce, labels, coleman,\n",
      "Nearest to day: waxes, procured, tractable, ceo, quiet, stanislaw, fta, nu,\n",
      "Nearest to what: goalkeepers, plausibly, postalveolar, usurp, compensation, stonework, cryptic, peuple,\n",
      "Nearest to into: between, in, from, zout, planner, accretion, boarder, of,\n",
      "Nearest to list: bawdy, artefact, sluggish, part, son, overlooking, edelman, sailor,\n",
      "Nearest to still: titus, widowers, engler, they, detours, ecmascript, ssc, simply,\n",
      "Nearest to power: bindings, pyrrhus, performed, silurian, totleben, holder, transplanted, mussels,\n",
      "Nearest to became: sprite, shusui, creationist, beech, humility, phosphates, moldavian, disturbing,\n",
      "Nearest to general: waffen, havenco, chime, salzburg, dough, outstripped, doorway, obsolete,\n",
      "Nearest to century: crustacean, granada, subversion, campus, bookstores, urine, robeson, trainer,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "data_index = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        batch_data, batch_labels = preprocessing.generate_batch(data,data_index,batch_size, num_skips, skip_window) \n",
    "        data_index = (data_index + batch_size) % len(data)\n",
    "        \n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "            \n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what an embedding looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11411995 -0.13313754  0.00158061 -0.05543647  0.07963617  0.21122105\n",
      "  0.20209837 -0.22767673  0.0210263  -0.08826434 -0.11811182  0.01635163\n",
      " -0.05881051 -0.03698393  0.02076265 -0.1300225  -0.10400225 -0.00245074\n",
      " -0.01343828 -0.0374172  -0.00809696  0.0406242  -0.02644057 -0.1182357\n",
      "  0.1065466   0.03270573 -0.04346969  0.08601567  0.04447291 -0.08904186\n",
      "  0.10633684 -0.11218706  0.01314771 -0.12314072  0.02663182 -0.02167294\n",
      " -0.07953636 -0.04160246 -0.02744665  0.10202218  0.15353282 -0.01898843\n",
      " -0.11432161  0.05285357 -0.05222141  0.15377945  0.05962488 -0.00351234\n",
      "  0.07354299  0.07420982  0.01105646 -0.16849099  0.06342104 -0.10554345\n",
      "  0.09934323  0.03107969  0.02193813  0.00610347 -0.00811313 -0.01340953\n",
      " -0.06066248 -0.16031983 -0.10002016 -0.15440792  0.02396211 -0.02666096\n",
      " -0.10059288 -0.02285783  0.02535136 -0.24538869 -0.11355957  0.06433243\n",
      " -0.05525658 -0.05198451  0.1519836  -0.02183996  0.09347224 -0.02050701\n",
      " -0.05416897 -0.02270798  0.1036659   0.07869019 -0.11427914 -0.01297607\n",
      "  0.03863408  0.04157366 -0.09820092 -0.05645463 -0.13370676  0.03683586\n",
      "  0.06021496 -0.05746422  0.08367988 -0.11451381  0.04237898 -0.05451999\n",
      "  0.04262164  0.0345896  -0.0685548   0.12084269  0.05903696  0.12972674\n",
      " -0.05975978 -0.12903413  0.22276483 -0.10145262  0.04717593 -0.14852579\n",
      " -0.04227746 -0.02513727 -0.04560398  0.09095202  0.02889668 -0.08828903\n",
      "  0.02483957  0.08104267 -0.08831523 -0.00892921 -0.06409584 -0.09296815\n",
      "  0.08313509 -0.03378777 -0.03935491  0.01458492  0.03758603  0.03567282\n",
      "  0.1419109   0.02821718]\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings have unit norm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.square(final_embeddings[40000,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we project the emmbeding vectors into a 2-dimensional space using [TSNE](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)\n",
    "\n",
    "We use the [TSNE sklearn implementation](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_points = 20\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=500)\n",
    "\n",
    "two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(15,15))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "words = [reverse_dictionary[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
